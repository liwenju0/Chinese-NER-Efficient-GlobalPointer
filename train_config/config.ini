[path]
model_path = hfl/chinese-roberta-wwm-ext
train_file_data = /Users/milter/Downloads/Chinese-NER-Efficient-GlobalPointer/data/cluener_public/train.json
val_file_data = /Users/milter/Downloads/Chinese-NER-Efficient-GlobalPointer/data/cluener_public/dev.json
model_save_path = multilabel_glob.pth
test_file_data = /Users/milter/Downloads/Chinese-NER-Efficient-GlobalPointer/data/cluener_public/test.json
out_file = cluener_test.json
[model_superparameter]
learning_rate = 2e-5
maxlen = 250
batch_size = 16
epochs = 10
head_size = 64
hidden_size = 768
inference_maxlen = 256
warmup_steps = 0
clip_norm = 0.25
abPosition = True
rePosition = False
max_relative = 127
re_maxlen = 512
dim_in = 768
dim_hid = 768
